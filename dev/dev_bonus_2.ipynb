{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import ast\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocNerBERT(nn.Module):\n",
    "    def __init__(self, num_doc_labels, num_token_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.doc_classifier = nn.Linear(self.bert.config.hidden_size, num_doc_labels)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, num_token_labels)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, doc_labels=None, token_labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        # Document classification\n",
    "        doc_output = outputs.pooler_output\n",
    "        doc_output = self.dropout(doc_output)\n",
    "        doc_logits = self.doc_classifier(doc_output)\n",
    "        \n",
    "        # Token classification\n",
    "        token_output = outputs.last_hidden_state\n",
    "        token_output = self.dropout(token_output)\n",
    "        token_logits = self.token_classifier(token_output)\n",
    "\n",
    "        if doc_labels is not None and token_labels is not None:\n",
    "            doc_loss = self.loss(doc_logits, doc_labels)\n",
    "            token_loss = self.loss(token_logits.view(-1, token_logits.shape[-1]), token_labels.view(-1))\n",
    "            loss = doc_loss + token_loss\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'doc_logits': doc_logits,\n",
    "            'token_logits': token_logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    texts = df['tokens'].apply(lambda x: ' '.join(ast.literal_eval(x))).tolist()\n",
    "    doc_labels = df['sentence_label'].tolist()\n",
    "    token_labels = df['ner_tags'].apply(ast.literal_eval).tolist()\n",
    "    \n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"doc_labels\": doc_labels,\n",
    "        \"token_labels\": token_labels\n",
    "    }\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer, max_length=128):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"token_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx] if word_idx < len(label) else -100)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"token_labels\"] = labels\n",
    "    tokenized_inputs[\"doc_labels\"] = examples[\"doc_labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    doc_logits, token_logits = logits\n",
    "    doc_labels, token_labels = labels\n",
    "\n",
    "    doc_predictions = np.argmax(doc_logits, axis=-1)\n",
    "    token_predictions = np.argmax(token_logits, axis=-1)\n",
    "\n",
    "    # Compute document classification accuracy\n",
    "    doc_correct = (doc_predictions == doc_labels).sum()\n",
    "    doc_total = len(doc_labels)\n",
    "    doc_accuracy = doc_correct / doc_total\n",
    "\n",
    "    # Compute token classification accuracy (ignoring padding tokens)\n",
    "    token_correct = ((token_predictions == token_labels) & (token_labels != -100)).sum()\n",
    "    token_total = (token_labels != -100).sum()\n",
    "    token_accuracy = token_correct / token_total\n",
    "\n",
    "    return {\n",
    "        \"doc_accuracy\": doc_accuracy,\n",
    "        \"token_accuracy\": token_accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv('train_augmented.csv')\n",
    "\n",
    "# Prepare data\n",
    "data = prepare_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4919.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda examples: tokenize_and_align_labels(examples, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize model\n",
    "num_doc_labels = len(df['sentence_label'].unique())\n",
    "num_token_labels = max(max(ast.literal_eval(x)) for x in df['ner_tags']) + 1\n",
    "model = DocNerBERT(num_doc_labels, num_token_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/375 [01:18<44:00,  7.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8495, 'grad_norm': 12.97952651977539, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/375 [02:27<40:58,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7017, 'grad_norm': 11.461528778076172, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 30/375 [03:37<39:56,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5959, 'grad_norm': 14.218560218811035, 'learning_rate': 3e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 40/375 [04:46<38:21,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3548, 'grad_norm': 12.133085250854492, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 50/375 [05:55<37:47,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0544, 'grad_norm': 10.131518363952637, 'learning_rate': 5e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 60/375 [07:05<36:28,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7357, 'grad_norm': 9.325644493103027, 'learning_rate': 6e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 70/375 [08:15<35:14,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4257, 'grad_norm': 10.599355697631836, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 80/375 [09:25<34:21,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2537, 'grad_norm': 7.2992072105407715, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 90/375 [10:41<33:18,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1955, 'grad_norm': 9.399588584899902, 'learning_rate': 9e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 100/375 [11:53<33:06,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1767, 'grad_norm': 9.410879135131836, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 110/375 [13:15<33:09,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8361, 'grad_norm': 9.77276611328125, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 120/375 [14:26<30:43,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.802, 'grad_norm': 11.23577880859375, 'learning_rate': 1.2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 130/375 [15:39<29:37,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8478, 'grad_norm': 8.7840576171875, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 140/375 [16:49<27:11,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7572, 'grad_norm': 6.267681121826172, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 150/375 [17:57<25:35,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4731, 'grad_norm': 5.59513521194458, 'learning_rate': 1.5e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 160/375 [19:05<24:27,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4271, 'grad_norm': 5.680927276611328, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 170/375 [20:16<23:37,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4987, 'grad_norm': 5.352529525756836, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 180/375 [21:25<22:23,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3541, 'grad_norm': 10.9909086227417, 'learning_rate': 1.8e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 190/375 [22:33<21:00,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3008, 'grad_norm': 11.67233943939209, 'learning_rate': 1.9e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 200/375 [23:46<21:23,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3435, 'grad_norm': 7.6928935050964355, 'learning_rate': 2e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 210/375 [24:57<19:20,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4332, 'grad_norm': 9.19049072265625, 'learning_rate': 2.1e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 220/375 [26:08<17:59,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2855, 'grad_norm': 13.92219352722168, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 230/375 [27:17<16:47,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.187, 'grad_norm': 4.876608371734619, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 240/375 [28:26<15:37,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1155, 'grad_norm': 12.742841720581055, 'learning_rate': 2.4e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [29:36<14:23,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2688, 'grad_norm': 7.570932865142822, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 260/375 [30:45<13:14,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.131, 'grad_norm': 21.518108367919922, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 270/375 [31:55<12:17,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9778, 'grad_norm': 6.184206485748291, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 280/375 [33:04<11:00,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1431, 'grad_norm': 11.963001251220703, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 290/375 [34:14<09:49,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1364, 'grad_norm': 15.883962631225586, 'learning_rate': 2.9e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 300/375 [35:25<09:19,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8667, 'grad_norm': 13.561271667480469, 'learning_rate': 3e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 310/375 [37:02<08:34,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9513, 'grad_norm': 12.236526489257812, 'learning_rate': 3.1e-05, 'epoch': 2.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 320/375 [38:12<06:26,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8017, 'grad_norm': 1.843321442604065, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 330/375 [39:21<05:10,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9124, 'grad_norm': 3.238100528717041, 'learning_rate': 3.3e-05, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 340/375 [40:30<04:01,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8481, 'grad_norm': 50.50942611694336, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 350/375 [41:40<02:55,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.901, 'grad_norm': 5.574615478515625, 'learning_rate': 3.5e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 360/375 [42:49<01:43,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1017, 'grad_norm': 16.127687454223633, 'learning_rate': 3.6e-05, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 370/375 [43:59<00:34,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9666, 'grad_norm': 17.190067291259766, 'learning_rate': 3.7e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [44:33<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2673.9341, 'train_samples_per_second': 1.122, 'train_steps_per_second': 0.14, 'train_loss': 1.6900064697265624, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./docnerbert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
